# Processing Strategy: Large Files (53K+ Lines)

## Overview
**File:** franz-kafka.txt (53,508 lines)  
**Output Structure:** Individual audio per work (22+ separate files)  
**Format:** `Kafka/Obra_Nombre/obra.mp3`  
**Strategy:** Split â†’ Process independently â†’ Isolate failures

---

## File Structure Detection

### Index Markers (User-Provided)
```
ÃNDICE
  AMÃ‰RICA (1912)
  LA METAMORFOSIS (1915)
  EL PROCESO (1925)
  ...
FIN DEL ÃNDICE

[Content of each work...]
```

### Output Organization
```
outputs/kafka/
â”œâ”€â”€ 01_America/
â”‚   â”œâ”€â”€ america_completo.mp3
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ chapters/
â”‚       â”œâ”€â”€ 01_capitulo.mp3
â”‚       â””â”€â”€ 02_capitulo.mp3
â”œâ”€â”€ 02_La_Metamorfosis/
â”‚   â””â”€â”€ metamorfosis_completo.mp3
â”œâ”€â”€ 03_El_Proceso/
â”‚   â””â”€â”€ proceso_completo.mp3
â””â”€â”€ [20+ more works]
```

**Benefits:**
- âœ… One corruption â‰  lost everything
- âœ… Process in any order
- âœ… Parallel work processing
- âœ… Easy quality check per work
- âœ… Resume specific works only

---

## Critical Problems & Solutions

### 1. Processing Time
- **Problem:** 200-800 hours sequential
- **Solution:** Process 22 works in parallel
- **Result:** ~2-4 hours (all works simultaneously)

### 2. Work Isolation
- **Problem:** Failure in work #15 breaks everything
- **Solution:** Independent processing per work
- **Recovery:** Re-run only failed works

### 3. Storage Management
- **Problem:** 400GB total WAV files
- **Solution:** Per-work compression + cleanup
- **Flow:** Work WAV â†’ MP3 â†’ Delete WAV â†’ Next work

### 4. Index Parsing
- **Problem:** Auto-detect work boundaries
- **Solution:** Use ÃNDICE/FIN markers + pattern matching
- **Validation:** User confirms detected works

### 5. Progress Tracking
- **Problem:** Which works are done?
- **Solution:** JSON manifest + Redis state
- **Status:** Real-time per-work progress

---

## Implementation Plan

### Phase 1: Parse & Validate (10 min)
```bash
# Extract index and validate structure
python -m modules.tts.cli parse franz-kafka.txt

# Output:
# âœ“ Found 22 works between ÃNDICE markers
# âœ“ Detected work boundaries
# âœ“ Generated: kafka_manifest.json
```

**Manifest Example:**
```json
{
  "total_works": 22,
  "works": [
    {
      "id": 1,
      "title": "AMÃ‰RICA",
      "year": 1912,
      "start_line": 150,
      "end_line": 5234,
      "estimated_duration": "2h 15m",
      "status": "pending"
    },
    {
      "id": 2,
      "title": "LA METAMORFOSIS",
      "year": 1915,
      "start_line": 5235,
      "end_line": 7890,
      "status": "pending"
    }
  ]
}
```

### Phase 2: Test Single Work (20 min)
```bash
# Process shortest work first (validation)
python -m modules.tts.cli process-work \
  --manifest kafka_manifest.json \
  --work-id 2 \
  --output outputs/kafka/

# Validates:
# - Parsing correctness
# - Audio quality
# - Merge logic
# - File organization
```

### Phase 3: Parallel Processing (2-8 hours)
```bash
# Process ALL works in parallel
python -m modules.tts.cli process-all \
  --manifest kafka_manifest.json \
  --workers 4 \
  --quality high

# Each work = independent Celery task group
# Failed works auto-retry 3 times
# Progress saved to Redis
```

### Phase 4: Verification (30 min)
```bash
# Check completion status
python -m modules.tts.cli status kafka_manifest.json

# Output:
# âœ“ Completed: 20/22 works
# âš  Failed: 2 works (IDs: 15, 18)
# â³ Processing: 0 works
# ðŸ“Š Total audio: 38h 22m (12.4 GB)

# Retry failed works
python -m modules.tts.cli retry-failed kafka_manifest.json
```

---

## Processing Strategy

### Per-Work Workflow
```
For each work in manifest:
  1. Extract text (lines X to Y)
  2. Detect chapters/sections
  3. Chunk text (500 chars)
  4. Parallel synthesis (Celery)
  5. Merge chunks â†’ chapters
  6. Merge chapters â†’ work.mp3
  7. Generate metadata.json
  8. Cleanup temp files
  9. Update manifest status
```

### Parallel Execution
```python
# 4 works processing simultaneously
Work 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80% (Chapter 8/10)
Work 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% âœ“ Complete
Work 3: [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 30% (Chapter 3/10)  
Work 4: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% Queued
```

### Failure Isolation
```
Work 15 fails â†’ Only Work 15 affected
Other 21 works â†’ Continue normally
Retry Work 15 â†’ Independent process
```

---

## File Organization

### Final Structure
```
outputs/kafka/
â”œâ”€â”€ _manifest.json              # Master status file
â”œâ”€â”€ _progress.json              # Real-time progress
â”‚
â”œâ”€â”€ 01_America_1912/
â”‚   â”œâ”€â”€ america.mp3             # Final output (2h 15m)
â”‚   â”œâ”€â”€ metadata.json           # Work info
â”‚   â””â”€â”€ cover.jpg               # Optional
â”‚
â”œâ”€â”€ 02_La_Metamorfosis_1915/
â”‚   â”œâ”€â”€ metamorfosis.mp3        # Final output (1h 30m)
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ 03_El_Proceso_1925/
â”‚   â”œâ”€â”€ proceso.mp3
â”‚   â””â”€â”€ metadata.json
â”‚
â””â”€â”€ [... 19 more works ...]
```

### Metadata Per Work
```json
{
  "title": "LA METAMORFOSIS",
  "author": "Franz Kafka",
  "year": 1915,
  "language": "es",
  "narrator": "Piper es_MX-claude-high",
  "duration": "1:32:45",
  "file_size": "425 MB",
  "chapters": 3,
  "generated": "2025-10-16T15:30:00Z",
  "enhancements": [
    "spectral_boost",
    "de-essing",
    "vocal_presence",
    "micro_humanization"
  ]
}
```

---

## Optimization Settings

### Quality Mode (Recommended)
```python
TTSEngine(
    language="es",
    enhance_quality=True
)
config = {
    "speed": 0.95,          # Slightly slower
    "chunk_size": 500,      # Optimal for quality
    "crossfade_ms": 100,    # Smooth transitions
    "workers": 8            # Parallel processing
}
```

### Fast Mode (Testing)
```python
TTSEngine(enhance_quality=False)
config = {
    "chunk_size": 1000,
    "workers": 16
}
```

---

## Monitoring & Recovery

### Real-time Dashboard
```bash
# Web interface
celery -A infrastructure.celery.celeryconfig flower
# â†’ http://localhost:5555

# CLI status
python -m modules.tts.cli status

# Output:
# ðŸ“Š Kafka Complete Works Processing
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# âœ“ Completed:  18/22 works (81%)
# â³ Processing: 3/22 works
# âŒ Failed:     1/22 works
# 
# Currently processing:
#   - Work 19: EL CASTILLO [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 65%
#   - Work 20: CARTA AL PADRE [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 35%
#   - Work 21: UN ARTISTA... [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 12%
#
# Estimated completion: 2h 15m
```

### Recovery Commands
```bash
# List failed works
python -m modules.tts.cli failed

# Retry specific work
python -m modules.tts.cli retry --work-id 15

# Retry all failed
python -m modules.tts.cli retry-all

# Resume interrupted processing
python -m modules.tts.cli resume
```

### Checkpointing
```json
// Redis: kafka:work:15:checkpoint
{
  "work_id": 15,
  "title": "UN MÃ‰DICO RURAL",
  "chunks_total": 456,
  "chunks_completed": 342,
  "last_chunk_id": "15_ch3_342",
  "last_updated": "2025-10-16T14:22:35Z",
  "can_resume": true
}
```

---

## Estimated Timeline

| Phase | Time | Description |
|-------|------|-------------|
| Parse & Validate | 10m | Extract index, create manifest |
| Test Single Work | 20m | Validate with shortest work |
| Infrastructure | 30m | Redis, MinIO, Celery setup |
| **Parallel Processing** | **2-8h** | All 22 works simultaneously |
| Verification | 30m | Check completion, retry failures |
| **Total** | **4-10h** | Complete collection (optimized) |

### Per-Work Estimates (Parallel)
```
Short works (1-2h audio):   30-60 min processing
Medium works (2-4h audio):  1-2 hours processing  
Long works (4-6h audio):    2-4 hours processing

With 4 parallel workers:
  Total sequential: 40-80 hours
  Actual time: 8-20 hours (4x faster)
  
With 8 parallel workers:
  Actual time: 4-10 hours (8x faster)
```

---

## Hardware Requirements

**Minimum (Sequential):**
- CPU: 4 cores
- RAM: 8GB
- Disk: 50GB free
- Time: 20-40h per complete run

**Recommended (4 works parallel):**
- CPU: 8 cores
- RAM: 16GB
- Disk: 100GB free (SSD)
- Time: 8-12h per complete run

**Optimal (8 works parallel):**
- CPU: 16 cores
- RAM: 32GB
- Disk: 200GB free (NVMe SSD)
- Time: 4-6h per complete run

---

## Quality Assurance

### Per-Work Validation
```bash
# Auto-check after each work completes
python -m modules.tts.cli validate --work-id 15

# Checks:
# âœ“ Audio file exists and playable
# âœ“ Duration matches expected
# âœ“ No corrupted segments
# âœ“ Consistent audio levels
# âœ“ Metadata generated
```

### Batch Validation
```bash
# After all processing
python -m modules.tts.cli validate-all

# Generates report:
# - Total duration: 38h 22m
# - Average quality score: 94/100
# - Files with issues: 0
# - Total size: 12.4 GB
```

---

## Next Steps

1. âœ… TTS engine complete with enhancements
2. âœ… Strategy documented
3. â³ **Implement manifest parser** (reads ÃNDICE markers)
4. â³ **Create work splitter** (extracts each work)
5. â³ **Build batch processor** (parallel execution)
6. â³ Test with La Metamorfosis
7. â³ Process complete collection

---

**Strategy:** Independent works â†’ Parallel processing â†’ Isolated failures  
**Priority:** Reliability > Speed > Quality  
**Approach:** Parse â†’ Test one â†’ Process all â†’ Validate
